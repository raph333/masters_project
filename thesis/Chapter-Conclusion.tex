\chapter{Conclusion}
\label{chapter:Conclusion}

In this master's project, I had the pleasure of investigating the exciting field of Message Passing Neural Networks (MPNNs) for chemical applications. Two different machine learning competitions~\footnote{\url{https://www.kaggle.com/c/champs-scalar-coupling}}~\footnote{\url{https://alchemy.tencent.com}} provided an excellent starting point for experimenting with different data representations and models to predict atomic interactions (Section~\ref{sec:champs}) or molecular properties (Section~\ref{sec:alchemy}). Furthermore, they allowed to benchmark the results against word-class machine learning practitioners and see their fascinating solutions after the competitions.

Graph neural networks are one of the most interesting frontiers of deep learning for a number of reasons. As stated in the Introduction (Section~\ref{sec:motivation}), most of the breakthroughs in deep learning occurred using image-data or natural language data (text and speech). Graph neural networks are one of the most promising methods for extending this success to other kinds of data. Another fascinating aspect of graph neural networks is that their applications encompass a lot of completely different fields from social networks, to documents and molecular structures. For this reason, the model-architectures are also required to be very diverse to fit the different use-cases.

This project investigated the challenges associated of using 3D data in general and molecular structures in particular with MPNNs. We have seen that incorporating 3D structural information into MPNNs in a way that increases the model's predictive power remains a big challenge (Section~\ref{sec:direction-vectors}). Furthermore, the results in this thesis support the pure deep learning approach of only using raw data as model input (Section~\ref{sec:raw-data}). However, the fact that manual feature engineering and raw data are still used alongside each other indicates that deep learning models for chemical applications are not yet as mature and efficient as in other fields (e.g. computer vision and natural language processing) where manual feature engineering has been made completely obsolete by end-to-end differentiable models using raw data only. Therefore, a lot of research remains to be done before MPNNs reach their full potential.

Finally, the project also showed that some of the key factors for the success of a machine learning project are not directly machine learning-related at first sight. One of them is data quality (or the lack thereof) which required a lot of attention in the project at had (Section~\ref{sec:diff-old-new-ds}). Furthermore, it is vital to make experiments easily reproducible, to be able to run multiple experiments automatically and to store all parameters and results in a systematic manner. To this end, the experimentation-process was largely automated in the project at hand~\footnote{The code is available on github: \url{https://github.com/raph333/masters_project}} and the python library \textit{mlflow} was found to be very useful to store and access all experiment-parameters and results. These factors are even more decisive in large organizations but provide substantial benefits in one-person projects as well.

Summing up, the project was a great learning experience for designing deep neural networks as well for structuring machine learning projects.