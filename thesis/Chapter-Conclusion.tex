\chapter{Conclusion}
\label{chapter:Conclusion}

In this master's project, I had the pleasure of investigating the exciting field of graph neural networks (MPNNs) for chemical applications. Two different machine learning competitions~\footnote{\url{https://www.kaggle.com/c/champs-scalar-coupling}}~\footnote{\url{https://alchemy.tencent.com}} provided an excellent starting point for thinking about suitable data representations and models to solve a given problem. Furthermore, they allowed to benchmark the results against word-class machine learning practitioners and investigate their fascinating solutions after the competitions.

Graph neural networks are one of the most interesting frontiers of deep learning for a number of reasons. As stated in the Introduction (Section~\ref{sec:motivation}), most of the breakthroughs in deep learning occurred using image-data or natural language data (both, text and speech). Graph neural networks are one of the most promising methods for extending this success for other kinds of data. Another fascinating aspect of graph neural networks is that their applications encompass a lot of completely different fields from social networks, to documents and molecular structures. For this reason, the model architectures are also required to be very diverse to fit the different use-cases.

This project investigated the challenges associated of modeling data in 3D space in general and molecular structures in particular with MPNNs. We have seen, that incorporating 3D structural information into MPNNs in way that the model extract features with high predictive power remains a big challenge (Section~\ref{sec:direction-vectors}). Furthermore, the results in this thesis support the pure deep learning approach of only using raw data as model input (Section~\ref{sec:raw-data}). However, the fact that manual feature engineering and raw data are still used alongside each other indicates that deep learning models for chemical applications are not yet as mature and efficient as in other fields (e.g. computer vision and natural language processing) where manual feature engineering has been made completely obsolete by models learning from raw data only.

Finally, the project also showed that some of the key factors for machine learning projects are not directly machine learning related at first sight. One being data quality (or the lack thereof) which severely hampered the progress in the project at had (Section~\ref{sec:diff-old-new-ds}). If the data is not reliable, a lot of effort is wasted on finding out whether the investigating data problems which impedes the project progress. Another key factor is to keep experiments reproducible, repeat experiments, being able to run several experiments with different parameters automatically and to keep a good overview of all experiment parameters and results. To this end, the process was largely automated in this project~\footnote{The code is available on github: \url{https://github.com/raph333/masters_project}} and the python library \textit{mlflow} was found to be very useful to log and save experiment parameters and results. These factors become even more decisive in large organizations but are already important in one-person projects.

Summing up, the project was a great learning experience for designing deep neural network architectures as well for structuring machine learning projects.