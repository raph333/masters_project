\chapter{Results}
\label{chapter:Results}


\section{Using graph convolution in the way it's meant to be used gives poor results on molecular structures}
\label{sec:neighborhood-expansion}

In the paper Chen et al.~\cite{Chen2019} at Tencent Lab, the authors modify a well performing GCNN architecture for quantum chemistry invented by Gilmer et al.~\cite{Gilmer2017}. The authors expand this model by using both, categorical bond type and euclidean distance as edge features and observe considerably better performance. At first glance, the use of both kinds of features - which seems to be implied in the paper - looks like a plausible explanation for the success. However, upon closer inspection, the reasoning does not add up. Bond types almost completely determine the euclidean distance. E.g. ever carbon-carbon single bond has a distance of 154Å, every carbon-oxygen double bond is 120Å long, etc.~\footnote{\url{https://courses.lumenlearning.com/suny-potsdam-organicchemistry/chapter/1-3-basics-of-bonding}} (The same applies to the reverse: knowing the distance between two atoms as well as the atom types allows to deduce the type of covalent bond or lack thereof.) Therefore, no additional information is added by including the euclidean distance as a feature. Inspection of the implementation reveals anther possible reason for the good performance of the Tencent Lab-paper: Not only is the euclidean distance added as a feature to existing edges, every atom is connected to every other atom in the molecule with the euclidean distance being the only not-null feature~\footnote{\url{https://github.com/tencent-alchemy/Alchemy}}. Thus, the structure of the graph is changed dramatically form the graph of covalent bonds to complete graph where every node is connected to every other node. The complete graph is a special case of graph and is rather untypical because in most applications, the presence or absence of an edge between two nodes is the most important kind of information. Simply connecting all nodes with each other seems to diminish this information (although in this case, the euclidean distance allows to distinguish between important and less important edges). With these considerations it is surprising but very interesting that the approach of Tencent Labs works so well.

In preparation for this thesis, I participated in the Tencent-Alchemy competition~\footnote{\url{https://alchemy.tencent.com}} using approach described above a baseline architecture. Despite many experiments, I achieved the best results simply fine-tuning Tencent Labs model and finished 26th out of 53 participants. The fact that half the participants did not beat the baseline published in the official competition paper, shows that the approach is indeed quite effective an warrants a closer examination why the peculiar approach of using complete graphs works quite well.

%As discussed in the introduction, a central concept of graph convolution (and regular convolution as well), is to compute a representation of a local neighborhood. In the case of graphs, a local neighborhood is a node and its neighbor nodes while on images, it is a small patch of pixels, such as 3x3 or 5x5.


As explained in the introduction, we can define the neighborhood of a node as all other nodes within a certain distance threshold. We refer to this threshold as neighborhood radius.

Therefore, the first experiment, we compare the learning curves of graphs when defining the neighborhood with different thresholds. First of all, covalent bonds are always represented as edges in the graph. Then, for increasing radii, edges to all non-covalently bonded atoms within the radius are added to the graph. A neighborhood radius of zero thus means that no additional edges are added to the graph - the graph only has edges between covalently bonded atoms. At a radius of 1.5 Ångström (1Å = 10nm), an edge is added between any two non-covalently bonded atoms with a distance of at most 1.5Å. The same is done for 2Å, etc. For perspective, most covalent bonds in organic molecules have a distance of around 1 - 1.5 Å~\footnote{\url{https://courses.lumenlearning.com/suny-potsdam-organicchemistry/chapter/1-3-basics-of-bonding}}. 1.5Å is very close for non-covalently bonded atoms such that only very few non-covalently bonded atoms - if any - will be within 1.5Å of an atom in an organic molecule. With a 2Å radius, every atom in the molecule will get some new neighbors due to this rule. With a 4Å radius, many atoms will be included in the neighborhood that have very little to no interaction with the central atom. At 5Å, for many small molecules, every atom is considered a neighbor of any other atom - applying this radius is therefore similar to connecting every atom with every other atom in the molecule. Finally, with an infinite radius, a complete graph is obtained.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/neighborhood-expansion}
	
	\caption{The learning curves with different neighborhood radii are show over the courses of 150 epochs. The color corresponds to the neighborhood radius. Dotted lines show the training- and solid lines the validation errors. Note that for each radius, there is a training- and a validation-error curve. Each curve is an average of three learning curves with the same neighborhood radius. All curves were generated using a simple optimization scheme described in Section~\ref{sec:training}. Both, validation- and training-error could be lowered considerably by employing a more sophisticated optimization scheme. However, this would be at the expense of comparability between curves (see Section~\ref{sec:training}).}
	\label{fig:neighborhood-expansion}
\end{figure}

As Figure~\ref{fig:neighborhood-expansion} shows, the best performance is indeed achieved with the complete graph - connecting every atom with every other atom in the molecule with an edge. In general, the larger the neighborhood radius, the better the performance~\footnote{Exception: radius 0}. This result is in line with the suspicion that the real reason for the good performance of Tencent Lab's model is the addition of new edges rather than the addition of the euclidean distance feature.
The result is somewhat counterintuitive and in contrast with the principle of (graph-)convolution. As described in the introduction, in theory, graph convolutions update node-representation based on the local neighborhood of each node. Eventually, after $t$ layers, each node representation would contain indirect contributions from all other nodes with $t$ or less edges between. However, the results at hand show that the performance is best if the graph convolution considers all atoms at every layer. This contrast between theory and empiric results clearly shows that much remains to be discovered about the use of GCNN for molecules.

For small molecules and with considerable computational power available, one could simply accept this finding as it is and represent molecules as complete graphs. However, for larger molecules with hundreds of atoms, this would be computationally not feasible as the number of edges increases quadratically ($\frac{n(n - 1)}{2}$ edges for $n$ nodes). Therefore, the next experiments will aim at finding an alternative way that captures the advantage of the complete graph-approach without the excessively high computational cost.

%Emphasize that this is not really appreciated in the literature.
%Cite Alchemy paper and point out that the high-performing kaggle-teams all use fully connected graphs.
%Does no one notice?


\section{Introducing a graph-state as added input to every graph-conv layer does not improve the results}


What the usage of complete graphs in graph convolutional networks essentially does is to ensures that during every node update, a representation of every other node in the graph contributes to the update - even nodes that are too far away to have any direct interaction with the updated node (atom).
A possible explanation of this behavior is that the node update benefits from having access to information about the whole graph / molecule. This interpretation begs the question if a similar effect can be achieved by a different architecture (that does not require the use of computationally prohibitive complete graphs).

An interesting experiment to test this hypothesis is to include graph-level information at every node update (without having to use complete graphs.) One such idea is the use of a root node. A root node is a virtual node that is connected to every other node in the graph. In the molecule case, the root node does not represent a real atom. Instead, it can be used to represent information about the whole molecule. Then, as every node is connected to the root node, every node update has access to information about the whole molecule. On could initialize the root node representation with molecule level features (such as number of atoms, electric dipole, etc.[CITATION]). However, all those features are implicit in the 3D molecular structure. Sticking to the pure deep learning philosophy, the root node representation can also be learned solely from the raw data.

Ideally, the use of such a root node would allow to achieve a similar performance as when using complete graphs without the high computational cost - while the number of edges increases quadratically with the number of nodes in the complete graph, the root node only introduces one new edge per node. At the very least, the effect of decreasing performance with decreasing neighborhood radius shown in Figure~\ref{fig:neighborhood-expansion} should be less pronounced if the root node has the desired effect.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/neighborhood-expansion-root-weight}
	\caption{The results illustrated by this figure have been produced in exactly the same manner is Figure~\ref{fig:neighborhood-expansion} with the addition of a root node as the only difference. This allows a direct comparison between the figures such that any difference could be attributed to the presence or absence of a root node. The results show no difference beyond stochastic variation thus indicating that the addition of a root node is no substitute to the use of complete graph representation of molecules.}
	\label{fig:neighborhood-expansion-root-weight}
\end{figure}

Figure~\ref{fig:neighborhood-expansion-root-weight} show the same experiment as Figure~\ref{fig:neighborhood-expansion} with the addition of a root node as the only difference. Unfortunately, there is no noticeable difference between the results of the two experiments. This indicates that the superior performance of complete graphs is not solely due to the fact that every node update includes input from the whole molecule. A possible reason for the failure of the root node to replace a complete graph approach could be that the root node cannot contribute geometric information about non-neighboring atoms. On the other hand, in the complete graph, each node's representation is considered in the update only in conjunction with the it's distance from the updated node. Recall Equation~\ref{eq:message-function} which defines the message form a node to another node as a function of both nodes as well as the connecting edge. While a root node representation can encode information about the whole molecule, such as number and types of atoms, their position relative to the updated node / atom is missing.





\section{Excluding hydrogen atoms from the molecule-graph speeds up training but reduces prediction accuracy}

Another possible preprocessing step is to disregard hydrogen atoms and instead add the number of hydrogen atoms bonded to a given heavy atom as a node feature. The rational behind this step is that the chemical properties of hydrogen atoms depend strongly on the heavy atom to which it is bound. While some information is lost during this step, the average number of atoms in the molecule is reduced by an average factor of
%21.61315346375882 ->
%9.712215362411802
%
%455.46524214239895 ->
%84.99518922386144
from 21.6 to 9.7 in the training-set. This modest reduction in the number of nodes leads to a very dramatic reduction in the number of edges. Naturally, this effect is most pronounced in complete graphs where the number of edges increases quadratically with the number of nodes ($\frac{n(n - 1)}{2}$ edges for $n$ nodes). While the average number of edges using complete graphs is 455.5 in the training-set, this number drops to 85 after excluding hydrogen atoms. Thus the data representation becomes much more compact leading to a dramatic decrease in training time (from around three minutes per epoch to only 0.9 minutes). This is a strong incentive to investigate whether ignoring hydrogen atoms in the molecular structure is a viable option. Figure~\ref{fig:implicit-hydrogens} clearly shows that this is not the case. The loss is far higher when using the hydrogen-less structure compared to the full structure. For this reason, every other experiment in this thesis uses full structures including hydrogen atoms.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/implict-hydrogens.pdf}
	
	\caption{}
	\label{fig:implicit-hydrogens}
\end{figure}

\paragraph{Further information: Why a hydrogen-less structure contains implicit information about hydrogen atoms}
Despite dropping all hydrogen atoms from the molecule structure, the information how many hydrogen atoms there are and to which atoms they are bound is still implicitly contained in the structure. In short, it is a way of summarizing raw data which speeds up training considerably while loosing some information. The reason it can be called 'implicit hydrogen representation' is that the information how many hydrogens (but not their exact position) is inherent in the structural information given all the other atoms and the bonds between them. The only piece of knowledge from organic chemistry required to understand this is that every given atom type, such as carbon or oxygen, only engages in a fixed number of bonds: four for carbon, two for oxygen, one for hydrogen, etc. (with double or triple bonds counting twice or trice, respectively). This number is called valence and is dependent on the number of electrons in the outer orbital - called valence electrons. In a molecular structure without hydrogen atoms, all bonds to heavy atoms (i.e. non-hydrogen atoms) is given for each atom as the number of it's neighbor nodes. The number of hydrogen atoms bound any given atom is thus it's valence minus the number of neighbor nodes. It is for this reason that omitting hydrogens from the structure can be thought of as equivalent of adding the feature 'number of hydrogen atoms' to each heavy atom. Note that while this number is implicit in the structure, the exact position of the hydrogen atoms is lost when they are omitted from the structure.



\section{graph-conv layers with independent weights give the same results as with shared weights}
\label{sec:weight-sharing}

Recall the message function $M_t$ and the (node-)update-function $U_t$ defined in Equation~\ref{eq:message-function} and~\ref{eq:update-function}, respectively. In this general definition, each graph-conv layer (each message passing step) $t$ has it's own $M_t$ and $U_t$. However, in most implementations of graph-conv networks, $M_t = M~~\forall~t$ and $U_t = U~~\forall~t$ which simplifies the two equations~\ref{eq:message-function} and~\ref{eq:update-function} to

\begin{equation}
m_v^{t+1} = \sum_{w \in N(v)} M(h_v^t, h_w^t, e_{vw})
\end{equation}
\label{eq:message-function-shared}

\begin{equation}
h_v^{t+1} = U(h_v^t, m_v^{t+1})
\end{equation}
\label{eq:update-function-shared}

% TODO: cite

In plain English, this means that the weights between the graph-conv layers (message passing steps) are shared. This allows for efficient training and works well in practice. However, there is no theoretical reason that weight sharing is necessarily superior. For instance, it would be conceivable that independent weights would work better for extracting different kinds of features at each graph-conv step. Recall that the graph-conv layer, the input for a given node update consists only of direct neighbor nodes, at the second layer, the update indirectly contains information from all second-degree neighbor nodes, etc. Hence, learning different weights for different graph-conv layers might be beneficial. Note also that in regular convolutional networks used for computer vision, every layer has independent weights even within blocks of convolutional layers with the same dimension. It is therefore far from clear that weight sharing has to be the preferred approach for graph-conv layers.

In order to investigate the possible benefit of independent weights, the next experiment compares the MPNN described in previous section with an architecture without weight sharing. The two architectures are identical in every other aspect to allow isolate the effect of weight sharing. Figure~\ref{fig:weight-sharing} shows that there is no noticeable difference between the learning curves.

Another interesting observation is that the learning curves are almost identical apart from the lower MAE of the model with independent weights. It would not have been surprising for the independent weights to require more training time as each graph-conv layers weights are only updated once for a given batch in a given epoch. In weight-sharing architectures on the other hand, the conv-layer is updated $n$ times for each batch and epoch in a model with $n$ conv-layers. Nevertheless, Figure~\ref{fig:weight-sharing} clearly shows that the two approaches are equal in terms of training-time.


One caveat worth mentioning is that only because independent weights were not found to be beneficial for the architecture at hand does not mean that they will never be beneficial in other architectures.

Furthermore, only because we know they do decrease the loss does not mean that that we know why exactly that's the case. This question is not addressed frequently in the literature - rather the use of weight-sharing is most commonly accepted without much investigation into why it's used. In contrast, we would not even think about sharing weights across regular conv-layers for a computer vision task. It is far from obvious why there is no difference between shared and independent weights in graph-conv networks. Finding out why this is the case might also lead insights on how to build better graph-conv architectures.  

Summing up the results confirm the common practice of using weight-sharing across graph-conv layers. The question why weight-sharing works in graph-conv nets still left open and worth investigating.



\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/weight-sharing.pdf}
	
	% todo update
	\caption{The result of using independent weights between graph-conv layers is the same as when using weight-shared layers. Note that the two architectures are identical in every other way apart from weight-sharing of graph-conv layers. Complete graphs were used as the input data as this was shown to be the best performing transformation in earlier experiments (Section~\ref{sec:neighborhood-expansion}). Each curve is an average of three learning curves with the same neighborhood radius. All curves were generated using a simple optimization scheme described in Section~\ref{sec:training}. While a more sophisticated optimization scheme and training for more epochs could have given slightly a somewhat lower MAE, the results a at hand show clearly the relative advantage of independent weights.}
	\label{fig:weight-sharing}
\end{figure}


\section{Manual feature engineering does not improve results compared to using only raw input data}
\label{sec:raw-data}

The previous experiments included some hand-crafted features calculated with the computation chemistry python library \textit{rdkit}. This was done in order replicate the approach of Chen et. at.~\cite{Chen2019} In the literature, there are conflicting [TODO cite] indications about the uses of manually engineered chemical features as input into graph-conv networks. In most cases, the literature lacks a direct comparison between using only raw data (atom-type, position and edges) and adding manually computed features. It is therefore often hard to deduce whether the good results are due to an effective graph-conv architecture or due the smartly engineered chemical features used as input.

The next experiment trains two almost identical graph-conv nets: one uses the same chemical features as Chen et. at.~\cite{Chen2019} while the other one uses only atom-type, edge-type and distance as input. The two nets are identical in every aspect except of course for the different dimension of the input layer. All chemical features~\footnote{TODO: add feature description} are computed from the structure without additional information. Hence - if they are relevant for predicting the target variable - they represent exactly the kind of information that the graph-conv layers should extract from the structure. If that's the case, the addition of the computed chemical features is expected to have no effect on the model's loss. Indeed, Figure~\ref{fig:raw-data} shows no difference between the two models.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/raw-data}
	\caption{Raw data alone gives as good results as with manually engineered features. As in the previous experiments, each line is an average of three independent learning curves and were obtained using the simple optimization scheme described in Section~\ref{sec:training}.}
	\label{fig:raw-data}
\end{figure}

Therefore, we can conclude that the graph-conv layers are able to extract relevant chemical information to predict the target variable. The results do not preclude that another architecture might benefit from the addition of computed chemical features or that more and more sophisticated manually engineered features might even provide some benefit for the model at hand. Nevertheless, the results show that manual feature engineering is far from essential and strengthen the believe that the deep-learning approach of using raw data instead of manual feature engineering is also preferable for chemical applications of graph-conv nets.


\section{Direction vectors as edge features have no impact}
\label{sec:direction-vectors}

It stands to reason that one of the main limitations of using graph neural networks for data embedded in spacial dimensions is the loss of a part of spacial information as explained in Section~\ref{sec:lack-of-3d-structure}. (While this theses focuses on molecules, representing any other three-dimensional data as graphs would be suffer from the same limitation).

In an attempt to address this limitation, an additional (vector-valued) edge-feature was included as input into the message-function $M_t$ (Equation~\ref{eq:message-function}): the direction vector from the sending atom $h_v$ to the receiving (i.e. to be updated) atom $h_w$ as added to the features of edge $e_{vw}$. This vector is invariant to translation of the molecule making it potentially more informative than using raw atom-coordinates as raw features~\footnote{Adding the raw coordinate values to the node features gave no improvement at all. As this is expected given that the coordinates depend on the arbitrary positioning and rotation of the coordinate system, the results are not shown here.} However, the direction vector is still not invariant the the arbitrarily chosen rotation of molecule in the coordinate system.

In order to provide the direction vector with some useful information the molecule's center was chosen as the origin of coordinate system. Thanks to this transformation, a direction vector $pos(h_w) - pos(h_v)$ from atom $h_v$ to atom $h_w$ always provided the information if $h_v$ lies closer to the center (positive values) or closer to the periphery of the molecule (negative values) in relation to $h_w$.

For this reason, the direction vector could contain useful information despite the lack of rotational invariance. Nevertheless, no improvement was achieved by the addition of the direction vector to the edge features as shown in Figure~\ref{fig:edge-direction-vectors}. The results do not give evidence whether the additional edge features do not contain relevant information at all or whether the graph-convolution was just not able to extract useful higher-level features from the information. The possibility that another grap-conv architecture might benefit from using direction vectors as edge-features cannot be ruled out completely but the results at hand do not provide evidence for it either.


\begin{figure}[H]
	\includegraphics[width=\linewidth]{figures/edge-direction-vectors}
	\caption{Direction vectors as edge features do note improve the results. As in the previous experiments, each line is an average of three independent learning curves and were obtained using the simple optimization scheme described in Section~\ref{sec:training}.}
	\label{fig:edge-direction-vectors}
\end{figure}


%optional:
%review the results of the 3DGCN-paper and show that this doesn't work really well on the Alchemy data - probably for the same reason given above









